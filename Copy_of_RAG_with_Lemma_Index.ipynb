{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3yfKimnElC11nHEPGeYt7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirupam15oct1/GRPO-First-Code/blob/main/Copy_of_RAG_with_Lemma_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b5tJbGp5GsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPEN_API_KEY ="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "2haroQ3k73NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZn76kQO-Fzy",
        "outputId": "255c245b-d348-4214-dc59-ec00c6a08ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.33-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.33 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.33.post1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.75.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.33->llama-index) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (3.11.15)\n",
            "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (9.1.2)\n",
            "Collecting tiktoken>=0.7.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (4.13.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.33->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.16-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.33->llama-index) (1.20.0)\n",
            "Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.33->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.33->llama-index) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.33->llama-index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.16 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.16-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.2.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.16->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.33->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.33->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.12.33-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.33.post1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.19-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.16-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.16-py3-none-any.whl (36 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, colorama, typing-inspect, tiktoken, griffe, llama-cloud, dataclasses-json, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed banks-2.1.2 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-cloud-0.1.19 llama-cloud-services-0.6.16 llama-index-0.12.33 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.33.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.38 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.16 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-5.4.0 python-dotenv-1.1.0 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "M_FpJ3-wyisp",
        "outputId": "94498f3a-1d79-4fcd-d903-ae6e15c7cec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4dcf9c8c-f548-41cd-9bee-42cadce11763\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4dcf9c8c-f548-41cd-9bee-42cadce11763\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MetaGPT.pdf to MetaGPT.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "doccuments=SimpleDirectoryReader(input_files=[\"MetaGPT.pdf\"]).load_data()\n"
      ],
      "metadata": {
        "id": "023BISZu-__a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "spilitter =SentenceSplitter(chunk_size=1024)\n",
        "nodes=spilitter.get_nodes_from_documents(doccuments)"
      ],
      "metadata": {
        "id": "7EJhCmsQ9Uw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embeded_model=OpenAIEmbedding(model=\"text-embedding-ada-002\")\n"
      ],
      "metadata": {
        "id": "FOXmSudxAmvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPEN_API_KEY\n",
        "from llama_index.core import SummaryIndex,VectorStoreIndex # Fixed the typo: VoetorStoreIndex to VectorStoreIndex\n",
        "summary_index=SummaryIndex(nodes)\n",
        "vector_index=VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "9-CQFvbfD1WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine=summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True\n",
        ")\n",
        "vector_query_engine=vector_index.as_query_engine()\n"
      ],
      "metadata": {
        "id": "pOzHw-AOHJ-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_tool=QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    )\n",
        "\n",
        ")\n",
        "vector_tool=QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from MetaGPT\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ix4pvBwPHmOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine=RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[summary_tool,vector_tool],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G-2e7L15JFMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"What is  the summary of the doccumnt?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0Im1jvyLW8N",
        "outputId": "ef9c7267-3a41-4972-901b-20681fd2dea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The summary of the document suggests that it is useful for summarization questions related to MetaGPT..\n",
            "\u001b[0mThe document details the development of MetaGPT, a meta-programming framework for multi-agent collaboration that incorporates Standardized Operating Procedures (SOPs) to enhance problem-solving capabilities. It involves role specialization, workflow management, and efficient communication protocols, along with an executable feedback mechanism to improve code generation quality during runtime. MetaGPT demonstrates state-of-the-art performance on various benchmarks, showcasing the benefits of SOPs in multi-agent collaborations. Additionally, the document discusses the development process within the MetaGPT platform, outlining user subscriptions, payment based on usage, and expanding agent capabilities. It covers the performance of MetaGPT in software development tasks, the impact of instruction levels on task outcomes, challenges such as reducing information overload and addressing code hallucinations, as well as limitations and ethics concerns related to unemployment, skill obsolescence, transparency, accountability, privacy, and data security.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3yCE_ExzJiL",
        "outputId": "94c1ee7a-e658-4aa8-f312-374e2cd94586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\n",
        "    \" How do agents share information with other agent\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf8LUH6P2qGj",
        "outputId": "fd8b25a3-c778-4392-9f33-d66fd9b637f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Sharing information with other agents requires retrieving specific context from MetaGPT.\n",
            "\u001b[0mAgents share information with other agents by utilizing a shared message pool. This shared message pool allows all agents to exchange messages directly. Agents publish their structured messages in the pool and can also access messages from other entities transparently. This method enables any agent to retrieve required information directly from the shared pool, eliminating the need to inquire about other agents and wait for their responses, thus enhancing communication efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool Calling\n"
      ],
      "metadata": {
        "id": "zeKfELEy3la5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "def add(x:int,y:int)->int:\n",
        "  \"\"\" Add two integer togather\"\"\"\n",
        "  return x+y\n",
        "\n",
        "def mystery(x:int,y:int)->int:\n",
        "  \"\"\"Mystery function operates on top of two numbers\"\"\"\n",
        "  return (x+y) + (x+y)\n",
        "\n",
        "add_tool= FunctionTool.from_defaults(fn=add)\n",
        "mystery_tool=FunctionTool.from_defaults(fn=mystery)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4yuhq5GJ3jdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
        "response=llm.predict_and_call(\n",
        "    [add_tool,mystery_tool],\n",
        "    \"Tell ne the oyput of the mystery function  on 2 and 9\", verbose=True\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQuoBuoZkjci",
        "outputId": "b2018fbc-a61b-4d4c-bf14-081fb2e5b715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
            "=== Function Output ===\n",
            "22\n",
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG with tool calling"
      ],
      "metadata": {
        "id": "KK_7RvppmUBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let understand the content of the Chunk\n",
        "print(nodes[0].get_content(metadata_mode='all'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-EuijJbl2ci",
        "outputId": "cfa271fb-fec4-4280-a16e-690704947f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_label: 1\n",
            "file_name: MetaGPT.pdf\n",
            "file_path: MetaGPT.pdf\n",
            "file_type: application/pdf\n",
            "file_size: 16753634\n",
            "creation_date: 2025-04-27\n",
            "last_modified_date: 2025-04-27\n",
            "\n",
            "Published as a conference paper at ICLR 2024\n",
            "METAGPT: M ETA PROGRAMMING FOR A\n",
            "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
            "Sirui Hong1∗, Mingchen Zhuge2∗, Jiaqi Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
            "Ceyao Zhang4, Jinlin Wang1, Zili Wang, Steven Ka Shing Yau5, Zijuan Lin4,\n",
            "Liyang Zhou6, Chenyu Ran1, Lingfeng Xiao1,7, Chenglin Wu1†, J¨urgen Schmidhuber2,8\n",
            "1DeepWisdom, 2AI Initiative, King Abdullah University of Science and Technology,\n",
            "3Xiamen University, 4The Chinese University of Hong Kong, Shenzhen,\n",
            "5Nanjing University, 6University of Pennsylvania,\n",
            "7University of California, Berkeley, 8The Swiss AI Lab IDSIA/USI/SUPSI\n",
            "ABSTRACT\n",
            "Remarkable progress has been made on automated problem solving through so-\n",
            "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
            "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
            "complex tasks, however, are complicated through logic inconsistencies due to\n",
            "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
            "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
            "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
            "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
            "streamlined workflows, thus allowing agents with human-like domain expertise\n",
            "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
            "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
            "complex tasks into subtasks involving many agents working together. On col-\n",
            "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
            "solutions than previous chat-based multi-agent systems. Our project can be found\n",
            "at https://github.com/geekan/MetaGPT.\n",
            "1 I NTRODUCTION\n",
            "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
            "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
            "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
            "Liang et al., 2023; Hao et al., 2023; Zhou et al., 2023b) tend to oversimplify the complexities. They\n",
            "struggle to achieve effective, coherent, and accurate problem-solving processes, particularly when\n",
            "there is a need for meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023a; Dong\n",
            "et al., 2023; Zhou et al., 2023a; Qian et al., 2023; Tang et al., 2023b; Hong et al., 2024).\n",
            "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
            "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
            "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
            "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
            "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
            "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
            "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
            "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
            "(PRDs) using a standardized structure, to guide the developmental process.\n",
            "Inspired by such ideas, we design a promising GPT -based Meta-Programming framework called\n",
            "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
            "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
            "∗These authors contributed equally to this work.\n",
            "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
            "1\n",
            "arXiv:2308.00352v7  [cs.AI]  1 Nov 2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "vector_index=VectorStoreIndex(nodes)\n",
        "query_engine=vector_index.as_query_engine(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "Usj1okr7nzUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters\n",
        "\n",
        "query_engine=vector_index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    filter=MetadataFilters.from_dicts(\n",
        "\n",
        "    [\n",
        "        {\"key\":\"page_lebel\",\"value\":\"2\"}\n",
        "\n",
        "    ]\n",
        "    )\n",
        ")\n",
        "\n",
        "response=query_engine.query(\n",
        "    \"What are some high-level results of MetaGPT\",\n",
        ")"
      ],
      "metadata": {
        "id": "xfxWH5m5pKdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGr3wtoIqWbR",
        "outputId": "f3323a8e-3036-4e5c-ebd9-746c96b3e2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some high-level results of MetaGPT include achieving an average score of 3.9, surpassing the score of ChatDev, and outperforming other general intelligent algorithms in generating executable code. Additionally, MetaGPT simplifies the process of transforming abstract requirements into detailed class and function designs, improving communication and code execution. Furthermore, MetaGPT demonstrates superior performance when collaborating with GPT-4 compared to other language models like GPT-3.5 and Deepseek Coder 33B.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "  print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ONKJj6qrnm",
        "outputId": "2ea2e724-552a-4b2c-c4ec-2dc8a115e3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '23', 'file_name': 'MetaGPT.pdf', 'file_path': 'MetaGPT.pdf', 'file_type': 'application/pdf', 'file_size': 16753634, 'creation_date': '2025-04-27', 'last_modified_date': '2025-04-27'}\n",
            "{'page_label': '7', 'file_name': 'MetaGPT.pdf', 'file_path': 'MetaGPT.pdf', 'file_type': 'application/pdf', 'file_size': 16753634, 'creation_date': '2025-04-27', 'last_modified_date': '2025-04-27'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Compact Code"
      ],
      "metadata": {
        "id": "V4WBFwwBsZOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import metadata\n",
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "\n",
        "def vector_query(\n",
        "    query:str,\n",
        "    page_numbers: List[str]\n",
        ")-> str:\n",
        "    \"\"\" Perform a Vector Search over an Index.\n",
        "\n",
        "    query{str} : the string query  to be embedded .\n",
        "    pagenumbers (List[str]): Filter by set of pages . LEAVE  BLANK if we  want  over all pages . Otehrwise , filter by set of specified pages .\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    metadata_dicts=[\n",
        "        {\"key\": \"page_label\",\"value\":p} for p in page_numbers\n",
        "    ]\n",
        "    query_engine=vector_index.as_query_engine(\n",
        "        similarity_top_k=2,\n",
        "        filter=MetadataFilters.from_dicts(\n",
        "            metadata_dicts,\n",
        "            condition=FilterCondition.OR\n",
        "        )\n",
        "    )\n",
        "    response=query_engine.query(query)\n",
        "    return str(response)\n",
        "\n",
        "vector_query_tool=FunctionTool.from_defaults(\n",
        "    name=\"vector_tool\",\n",
        "    fn=vector_query\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "hpZghVmysWy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
        "response=llm.predict_and_call(\n",
        "    [vector_query_tool],\n",
        "    \"What are the high level rsults of MetaGPT as described on Page 2 ?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkP1XeObvs3w",
        "outputId": "81419362-a7c7-4df6-b947-1a4d4c697dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"high level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT achieves high-level results by outperforming all preceding approaches in both HumanEval and MBPP benchmarks. It demonstrates superior performance when collaborating with GPT-4, significantly improving the Pass @k in the HumanEval benchmark compared to GPT-4. The pass rates for MetaGPT on the MBPP and HumanEval benchmarks are 85.9% and 87.7%, respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_index=SummaryIndex(nodes)\n",
        "summary_query_engine=summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True\n",
        ")\n",
        "summary_tool=QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "JI1f1pYUws0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm.predict_and_call(\n",
        "    [vector_query_tool,summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev distributed page 8 ?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlxkjHapyZxr",
        "outputId": "625a928e-fa39-4b9a-f8c8-8a3174857861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT outperforms ChatDev on the SoftwareDev dataset in various aspects such as executability, running times, token usage, code statistics, productivity, and human revision cost. MetaGPT achieves a higher score in executability, takes less time to generate code, uses more tokens but is more efficient in generating code lines, and has lower human revision costs compared to ChatDev. Additionally, MetaGPT demonstrates autonomous software generation capabilities and highlights the benefits of using SOPs in collaborations between multiple agents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agentic frame work which ans wer complex Qiestion"
      ],
      "metadata": {
        "id": "5h4TvvtmzpSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent  import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import  AgentRunner\n",
        "\n",
        "agent_worker=FunctionCallingAgentWorker.from_tools(\n",
        "    [vector_tool,summary_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent=AgentRunner(agent_worker)\n"
      ],
      "metadata": {
        "id": "akOA-uEkz3AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=agent.query(\n",
        "    \"Tell   me about the agent role in MetaGPT,\"\n",
        "    'and then how they communicate with each other '\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBiTedoF1aeA",
        "outputId": "7b498fc0-115a-49b5-e9b2-4f666f7a2de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell   me about the agent role in MetaGPT,and then how they communicate with each other \n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"What is the agent role in MetaGPT?\"}\n",
            "=== Function Output ===\n",
            "The agent role in MetaGPT encompasses a range of responsibilities within the software development process, including generating product requirement documents, designing system architecture, breaking down projects into tasks, executing designated classes and functions, formulating test cases, and ensuring high-quality code through code reviews and bug identification. Each agent within MetaGPT performs specific functions that contribute to the overall software development process.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"How do agents communicate with each other in MetaGPT?\"}\n",
            "=== Function Output ===\n",
            "Agents in MetaGPT communicate with each other through structured communication interfaces, a publish-subscribe mechanism, a shared message pool, and a subscription mechanism. This approach allows them to exchange structured messages efficiently, publish messages in a shared pool, access messages from other agents, and subscribe to specific information based on their role profiles.\n",
            "=== LLM Response ===\n",
            "The agent role in MetaGPT involves various responsibilities in software development, such as generating product requirement documents, designing system architecture, and ensuring code quality. Agents in MetaGPT communicate with each other through structured communication interfaces, a publish-subscribe mechanism, a shared message pool, and a subscription mechanism.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building agentic framework with much more user control"
      ],
      "metadata": {
        "id": "pJexfW6r3t6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task=agent.create_task(\n",
        "    \"Tell me about the agent role in MetaGPT,\"\n",
        "    'and then how they communicate with each other '\n",
        ")\n"
      ],
      "metadata": {
        "id": "A0qKQ3PA3r70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_outputs=agent.run_step(task.task_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV1GF7p04o3G",
        "outputId": "2be5f9cb-2d79-44f5-8a47-483126942a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the agent role in MetaGPT,and then how they communicate with each other \n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"What is the agent role in MetaGPT?\"}\n",
            "=== Function Output ===\n",
            "The agent roles in MetaGPT encompass a range of responsibilities within the software development process, including generating product requirement documents, designing system architecture, breaking down projects into tasks, executing designated classes and functions, formulating test cases, and ensuring stringent code quality through collaborative efforts.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"How do agents communicate with each other in MetaGPT?\"}\n",
            "=== Function Output ===\n",
            "Agents in MetaGPT communicate with each other through structured communication interfaces, a publish-subscribe mechanism, and a shared message pool. This allows them to exchange structured messages directly, publish messages in the pool, and access messages from other agents transparently. Agents can also subscribe to specific information based on their role profiles to ensure they receive only relevant information. This structured communication approach enhances efficiency and reduces the risk of information overload during collaboration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completed_steps=agent.get_completed_steps(task.task_id)\n",
        "print(f\"Num completed for task{task.task_id} {len(completed_steps)}\")\n",
        "print(completed_steps[0].output.sources[0].raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEU-QLlK5OBK",
        "outputId": "19432e35-e5f9-43b3-9d7d-0e514439e997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num completed for task3cebd300-e8a1-4225-9481-5726210e1648 1\n",
            "The agent roles in MetaGPT encompass a range of responsibilities within the software development process, including generating product requirement documents, designing system architecture, breaking down projects into tasks, executing designated classes and functions, formulating test cases, and ensuring stringent code quality through collaborative efforts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upcoming_stpes=agent.get_upcoming_steps(task.task_id)\n",
        "print(f\"Num upcoming steps for task {task.task_id} {len(upcoming_stpes)}\")\n",
        "upcoming_stpes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPiI7Yd76r5K",
        "outputId": "3d0fef48-6868-49f5-99e8-c5d643ca5f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num upcoming steps for task 3cebd300-e8a1-4225-9481-5726210e1648 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskStep(task_id='3cebd300-e8a1-4225-9481-5726210e1648', step_id='3e2af430-9cf6-4ff1-aeb0-013f33f8cf04', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}